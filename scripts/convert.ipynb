{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08a5bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: luobai\n",
    "Date: 2025-08-09\n",
    "Description:\n",
    "This script processes AgiBotWorld datasets and converts them into the LeRobot dataset format(adapt to the aloha dataset\n",
    "used by physical robots team,ref:https://huggingface.co/datasets/physical-intelligence/aloha_pen_uncap_diverse).\n",
    "And after convert dataset,you can use the privious codes to train the model.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gc\n",
    "import shutil\n",
    "from concurrent.futures import (\n",
    "    ThreadPoolExecutor,\n",
    "    as_completed,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from agibot_utils.agibot_utils import get_task_info, load_local_dataset\n",
    "from agibot_utils.config import AgiBotWorld_TASK_TYPE\n",
    "from agibot_utils.lerobot_utils import compute_episode_stats, generate_features_from_config\n",
    "from lerobot.datasets.compute_stats import aggregate_stats\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.datasets.utils import (\n",
    "    check_timestamps_sync,\n",
    "    get_episode_data_index,\n",
    "    validate_episode_buffer,\n",
    "    validate_frame,\n",
    "    write_episode,\n",
    "    write_episode_stats,\n",
    "    write_info,\n",
    ")\n",
    "from lerobot.datasets.video_utils import get_safe_default_codec\n",
    "from ray.runtime_env import RuntimeEnv\n",
    "\n",
    "import os\n",
    "import tyro\n",
    "class AgiBotDatasetMetadata(LeRobotDatasetMetadata):\n",
    "    def save_episode(\n",
    "        self,\n",
    "        episode_index: int,\n",
    "        episode_length: int,\n",
    "        episode_tasks: list[str],\n",
    "        episode_stats: dict[str, dict],\n",
    "        action_config: list[dict],\n",
    "    ) -> None:\n",
    "        self.info[\"total_episodes\"] += 1\n",
    "        self.info[\"total_frames\"] += episode_length\n",
    "\n",
    "        chunk = self.get_episode_chunk(episode_index)\n",
    "        if chunk >= self.total_chunks:\n",
    "            self.info[\"total_chunks\"] += 1\n",
    "\n",
    "        self.info[\"splits\"] = {\"train\": f\"0:{self.info['total_episodes']}\"}\n",
    "        self.info[\"total_videos\"] += len(self.video_keys)\n",
    "        if len(self.video_keys) > 0:\n",
    "            self.update_video_info()\n",
    "\n",
    "        write_info(self.info, self.root)\n",
    "\n",
    "        episode_dict = {\n",
    "            \"episode_index\": episode_index,\n",
    "            \"tasks\": episode_tasks,\n",
    "            \"length\": episode_length,\n",
    "            \"action_config\": action_config,\n",
    "        }\n",
    "        self.episodes[episode_index] = episode_dict\n",
    "        write_episode(episode_dict, self.root)\n",
    "\n",
    "        self.episodes_stats[episode_index] = episode_stats\n",
    "        self.stats = aggregate_stats([self.stats, episode_stats]) if self.stats else episode_stats\n",
    "        write_episode_stats(episode_index, episode_stats, self.root)\n",
    "\n",
    "\n",
    "class AgiBotDataset(LeRobotDataset):\n",
    "    @classmethod\n",
    "    def create(\n",
    "        cls,\n",
    "        repo_id: str,\n",
    "        fps: int,\n",
    "        features: dict,\n",
    "        root: str | Path | None = None,\n",
    "        robot_type: str | None = None,\n",
    "        use_videos: bool = True,\n",
    "        tolerance_s: float = 1e-4,\n",
    "        image_writer_processes: int = 0,\n",
    "        image_writer_threads: int = 0,\n",
    "        video_backend: str | None = None,\n",
    "    ) -> \"LeRobotDataset\":\n",
    "        \"\"\"Create a LeRobot Dataset from scratch in order to record data.\"\"\"\n",
    "        obj = cls.__new__(cls)\n",
    "        obj.meta = AgiBotDatasetMetadata.create(\n",
    "            repo_id=repo_id,\n",
    "            fps=fps,\n",
    "            robot_type=robot_type,\n",
    "            features=features,\n",
    "            root=root,\n",
    "            use_videos=use_videos,\n",
    "        )\n",
    "        obj.repo_id = obj.meta.repo_id\n",
    "        obj.root = obj.meta.root\n",
    "        obj.revision = None\n",
    "        obj.tolerance_s = tolerance_s\n",
    "        obj.image_writer = None\n",
    "\n",
    "        if image_writer_processes or image_writer_threads:\n",
    "            obj.start_image_writer(image_writer_processes, image_writer_threads)\n",
    "\n",
    "        # TODO(aliberts, rcadene, alexander-soare): Merge this with OnlineBuffer/DataBuffer\n",
    "        obj.episode_buffer = obj.create_episode_buffer()\n",
    "\n",
    "        obj.episodes = None\n",
    "        obj.hf_dataset = obj.create_hf_dataset()\n",
    "        obj.image_transforms = None\n",
    "        obj.delta_timestamps = None\n",
    "        obj.delta_indices = None\n",
    "        obj.episode_data_index = None\n",
    "        obj.video_backend = video_backend if video_backend is not None else get_safe_default_codec()\n",
    "        return obj\n",
    "\n",
    "    def add_frame(self, frame: dict, task: str, timestamp: float | None = None) -> None:\n",
    "        \"\"\"\n",
    "        This function only adds the frame to the episode_buffer. Apart from images — which are written in a\n",
    "        temporary directory — nothing is written to disk. To save those frames, the 'save_episode()' method\n",
    "        then needs to be called.\n",
    "        \"\"\"\n",
    "        # Convert torch to numpy if needed\n",
    "        for name in frame:\n",
    "            if isinstance(frame[name], torch.Tensor):\n",
    "                frame[name] = frame[name].numpy()\n",
    "\n",
    "        features = {key: value for key, value in self.features.items() if key in self.hf_features}  # remove video keys\n",
    "        validate_frame(frame, features)\n",
    "\n",
    "        if self.episode_buffer is None:\n",
    "            self.episode_buffer = self.create_episode_buffer()\n",
    "\n",
    "        # Automatically add frame_index and timestamp to episode buffer\n",
    "        frame_index = self.episode_buffer[\"size\"]\n",
    "        if timestamp is None:\n",
    "            timestamp = frame_index / self.fps\n",
    "        self.episode_buffer[\"frame_index\"].append(frame_index)\n",
    "        self.episode_buffer[\"timestamp\"].append(timestamp)\n",
    "        self.episode_buffer[\"task\"].append(task)\n",
    "\n",
    "        # Add frame features to episode_buffer\n",
    "        for key, value in frame.items():\n",
    "            if key not in self.features:\n",
    "                raise ValueError(\n",
    "                    f\"An element of the frame is not in the features. '{key}' not in '{self.features.keys()}'.\"\n",
    "                )\n",
    "\n",
    "            self.episode_buffer[key].append(value)\n",
    "\n",
    "        self.episode_buffer[\"size\"] += 1\n",
    "\n",
    "    def save_episode(self, videos: dict, action_config: list, episode_data: dict | None = None) -> None:\n",
    "        \"\"\"\n",
    "        This will save to disk the current episode in self.episode_buffer.\n",
    "\n",
    "        Args:\n",
    "            episode_data (dict | None, optional): Dict containing the episode data to save. If None, this will\n",
    "                save the current episode in self.episode_buffer, which is filled with 'add_frame'. Defaults to\n",
    "                None.\n",
    "        \"\"\"\n",
    "        if not episode_data:\n",
    "            episode_buffer = self.episode_buffer\n",
    "\n",
    "        validate_episode_buffer(episode_buffer, self.meta.total_episodes, self.features)\n",
    "\n",
    "        # size and task are special cases that won't be added to hf_dataset\n",
    "        episode_length = episode_buffer.pop(\"size\")\n",
    "        tasks = episode_buffer.pop(\"task\")\n",
    "        episode_tasks = list(set(tasks))\n",
    "        episode_index = episode_buffer[\"episode_index\"]\n",
    "\n",
    "        episode_buffer[\"index\"] = np.arange(self.meta.total_frames, self.meta.total_frames + episode_length)\n",
    "        episode_buffer[\"episode_index\"] = np.full((episode_length,), episode_index)\n",
    "\n",
    "        # Add new tasks to the tasks dictionary\n",
    "        for task in episode_tasks:\n",
    "            task_index = self.meta.get_task_index(task)\n",
    "            if task_index is None:\n",
    "                self.meta.add_task(task)\n",
    "\n",
    "        # Given tasks in natural language, find their corresponding task indices\n",
    "        episode_buffer[\"task_index\"] = np.array([self.meta.get_task_index(task) for task in tasks])\n",
    "\n",
    "        for key, ft in self.features.items():\n",
    "            # index, episode_index, task_index are already processed above, and image and video\n",
    "            # are processed separately by storing image path and frame info as meta data\n",
    "            if key in [\"index\", \"episode_index\", \"task_index\"] or ft[\"dtype\"] in [\"video\"]:\n",
    "                continue\n",
    "            episode_buffer[key] = np.stack(episode_buffer[key]).squeeze()\n",
    "\n",
    "        for key in self.meta.video_keys:\n",
    "            video_path = self.root / self.meta.get_video_file_path(episode_index, key)\n",
    "            episode_buffer[key] = str(video_path)  # PosixPath -> str\n",
    "            video_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copyfile(videos[key], video_path)\n",
    "\n",
    "        ep_stats = compute_episode_stats(episode_buffer, self.features)\n",
    "\n",
    "        self._save_episode_table(episode_buffer, episode_index)\n",
    "\n",
    "        # `meta.save_episode` be executed after encoding the videos\n",
    "        # add action_config to current episode\n",
    "        self.meta.save_episode(episode_index, episode_length, episode_tasks, ep_stats, action_config)\n",
    "\n",
    "        ep_data_index = get_episode_data_index(self.meta.episodes, [episode_index])\n",
    "        ep_data_index_np = {k: t.numpy() for k, t in ep_data_index.items()}\n",
    "        check_timestamps_sync(\n",
    "            episode_buffer[\"timestamp\"],\n",
    "            episode_buffer[\"episode_index\"],\n",
    "            ep_data_index_np,\n",
    "            self.fps,\n",
    "            self.tolerance_s,\n",
    "        )\n",
    "\n",
    "        if not episode_data:  # Reset the buffer\n",
    "            self.episode_buffer = self.create_episode_buffer()\n",
    "\n",
    "\n",
    "def get_all_tasks(src_path: Path, output_path: Path):\n",
    "    json_files = src_path.glob(\"task_info/*.json\")\n",
    "    print(f\"Found {len(list(json_files))} task_info files in {src_path}\")\n",
    "    for json_file in json_files:\n",
    "        local_dir = output_path / \"agibotworld\" / json_file.stem\n",
    "        yield (json_file, local_dir.resolve())\n",
    "\n",
    "# DatasetLists: list[Dataset] = []\n",
    "def get_all_tasks_sim(src_path: Path, output_path: Path):\n",
    "    for root, dirs, files in os.walk(src_path):\n",
    "            # print(f\"Checking folder: {root}\")\n",
    "        if(\"task_train.json\" not in files):\n",
    "            continue\n",
    "        json_file = Path(root) / \"task_train.json\"\n",
    "        local_dir = output_path / \"agibotworld\" / \"sim\" / Path(root).name\n",
    "        if not local_dir.exists():\n",
    "            local_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # 处理的任务ID不对\n",
    "        yield (json_file, local_dir.resolve())\n",
    "\n",
    "def save_as_lerobot_dataset(agibot_world_config, task: tuple[Path, Path], num_threads, save_depth, debug):\n",
    "    json_file, local_dir = task\n",
    "    # task_id= \"2810137\"  # TODO: FIXid\n",
    "    src_path = json_file.parent.parent\n",
    "    task_info = get_task_info(json_file)\n",
    "    task_id = task_info[0][\"task_id\"]\n",
    "    task_name = task_info[0][\"task_name\"]\n",
    "    print(f\"processing {task_name}, saving to {local_dir}\")\n",
    "    \n",
    "    task_init_scene = task_info[0][\"init_scene_text\"]\n",
    "    task_instruction = f\"{task_name} | {task_init_scene}\"\n",
    "    # task_id = json_file.stem.split(\"_\")[-1]\n",
    "    task_info = {episode[\"episode_id\"]: episode for episode in task_info}\n",
    "\n",
    "    features = generate_features_from_config(agibot_world_config)\n",
    "\n",
    "    if local_dir.exists():\n",
    "        shutil.rmtree(local_dir)\n",
    "\n",
    "    # print(f'features: {features}')\n",
    "    # if not save_depth:\n",
    "    #     features.pop(\"observation.images.head_depth\")\n",
    "\n",
    "    dataset: AgiBotDataset = AgiBotDataset.create(\n",
    "        repo_id= f\"{task_name}\",\n",
    "        root=local_dir,\n",
    "        fps=30,\n",
    "        robot_type=\"a2d\",\n",
    "        features=features,\n",
    "    )\n",
    "    all_subdir=[]\n",
    "    for root, dirs, files in os.walk(src_path):\n",
    "        if \"data_info.json\" in files:\n",
    "            job_ids=agibot_world_config['job_ids']\n",
    "            if(Path(root).name in job_ids):\n",
    "                # 只有我们筛选过合格的才加进去\n",
    "                all_subdir += [Path(root)]\n",
    "\n",
    "    # print(f\"Found {len(all_subdir)} subdirectories in {src_path / 'observations' / task_id}\")\n",
    "    print(f\"Found {len(all_subdir)} subdirectories in {src_path}\")\n",
    "    # all_subdir_eids = [int(subdir.name.split(\"_\")[-1]) for subdir in all_subdir if subdir.name.startswith(\"episode_\")]\n",
    "    all_subdir_eids = sorted([int(path.name) for path in all_subdir])\n",
    "    all_subdir_dict = {path.name: path for path in all_subdir}\n",
    "    print(f\"Found {len(all_subdir_eids)} episode IDs in {src_path}\")\n",
    "    if debug or not save_depth:\n",
    "        for eid in all_subdir_eids:\n",
    "            # eid 存的是jobid\n",
    "            if eid not in task_info:\n",
    "                print(f\"{json_file.stem}, episode_{eid} not in task_info.json, skipping...\")\n",
    "                continue\n",
    "            print(f\"Processing episode_{eid} in {json_file.stem}\")\n",
    "            action_config = task_info[eid][\"label_info\"][\"action_config\"]\n",
    "            src_path_use= all_subdir_dict[f\"{eid}\"]\n",
    "            raw_dataset = load_local_dataset(\n",
    "                eid,\n",
    "                src_path=str(src_path_use[f'{eid}']),\n",
    "                task_id=task_id,\n",
    "                save_depth=save_depth,\n",
    "                AgiBotWorld_CONFIG=agibot_world_config,\n",
    "            )\n",
    "            _, frames, videos = raw_dataset\n",
    "            \n",
    "            if not all([video_path.exists() for video_path in videos.values()]):\n",
    "                print(f\"{json_file.stem}, episode_{eid}: some of the videos does not exist, skipping...\")\n",
    "                continue\n",
    "\n",
    "            for frame_data in frames:\n",
    "                dataset.add_frame(frame_data, task_instruction)\n",
    "            try:\n",
    "                dataset.save_episode(videos=videos, action_config=action_config)\n",
    "            except Exception as e:\n",
    "                print(f\"{json_file.stem}, episode_{eid}: there are some corrupted mp4s\\nException details: {str(e)}\")\n",
    "                dataset.episode_buffer = None\n",
    "                continue\n",
    "            gc.collect()\n",
    "            print(f\"process done for {json_file.stem}, episode_id {eid}, len {len(frames)}\")\n",
    "    else:\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            futures = []\n",
    "            for eid in all_subdir_eids:\n",
    "                if eid not in task_info:\n",
    "                    print(f\"{json_file.stem}, episode_{eid} not in task_info.json, skipping...\")\n",
    "                    continue\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        load_local_dataset,\n",
    "                        eid,\n",
    "                        src_path=src_path,\n",
    "                        task_id=task_id,\n",
    "                        save_depth=save_depth,\n",
    "                        AgiBotWorld_CONFIG=agibot_world_config,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            for raw_dataset in as_completed(futures):\n",
    "                eid, frames, videos = raw_dataset.result()\n",
    "                if not all([video_path.exists() for video_path in videos.values()]):\n",
    "                    print(f\"{json_file.stem}, episode_{eid}: some of the videos does not exist, skipping...\")\n",
    "                    continue\n",
    "                action_config = task_info[eid][\"label_info\"][\"action_config\"]\n",
    "                for frame_data in frames:\n",
    "                    dataset.add_frame(frame_data, task_instruction)\n",
    "                try:\n",
    "                    dataset.save_episode(videos=videos, action_config=action_config)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"{json_file.stem}, episode_{eid}: there are some corrupted mp4s\\nException details: {str(e)}\"\n",
    "                    )\n",
    "                    dataset.episode_buffer = None\n",
    "                    continue\n",
    "                gc.collect()\n",
    "                print(f\"process done for {json_file.stem}, episode_id {eid}, len {len(frames)}\")\n",
    "\n",
    "\n",
    "def main(\n",
    "    src_path: str,\n",
    "    output_path: str,\n",
    "    eef_type: str,\n",
    "    task_ids: list,\n",
    "    cpus_per_task: int,\n",
    "    num_threads_per_task: int,\n",
    "    save_depth: bool,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    #print(f'debug mode: {debug}, save_depth: {save_depth}')\n",
    "    tasks = list(get_all_tasks_sim(src_path, output_path)) # 原来返回的是一次性迭代器\n",
    "    print(f\"Total tasks found: {len(tasks)}\")\n",
    "\n",
    "    agibot_world_config, type_task_ids = (\n",
    "        AgiBotWorld_TASK_TYPE[eef_type][\"task_config\"],\n",
    "        AgiBotWorld_TASK_TYPE[eef_type][\"task_ids\"],\n",
    "    )\n",
    "    # sim 不用筛选\n",
    "    if eef_type == \"sim\":\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Using AgiBotWorld config for {eef_type}: {agibot_world_config}\")\n",
    "        if eef_type == \"gripper\":\n",
    "            remaining_ids = AgiBotWorld_TASK_TYPE[\"dexhand\"][\"task_ids\"] + AgiBotWorld_TASK_TYPE[\"tactile\"][\"task_ids\"]\n",
    "            tasks = filter(lambda task: task[0].stem not in remaining_ids, tasks)\n",
    "        else:\n",
    "            tasks = filter(lambda task: task[0].stem in type_task_ids, tasks)\n",
    "\n",
    "        if task_ids:\n",
    "            tasks = filter(lambda task: task[0].stem in task_ids, tasks)\n",
    "    save_depth=False if eef_type == \"sim\" else save_depth\n",
    "    if debug:\n",
    "        # print(f\"Debug mode enabled, processing only the first task: {tasks}\")\n",
    "        save_as_lerobot_dataset(agibot_world_config,tasks[0], num_threads_per_task, save_depth, debug)\n",
    "    else:\n",
    "        runtime_env = RuntimeEnv(\n",
    "            env_vars={\n",
    "                \"HDF5_USE_FILE_LOCKING\": \"FALSE\",\n",
    "                \"HF_DATASETS_DISABLE_PROGRESS_BARS\": \"TRUE\",\n",
    "                \"LD_PRELOAD\": str(Path(__file__).resolve().parent / \"libtcmalloc.so.4.5.3\"),\n",
    "            }\n",
    "        )\n",
    "        ray.init(runtime_env=runtime_env)\n",
    "        resources = ray.available_resources()\n",
    "        cpus = int(resources[\"CPU\"])\n",
    "        # print(f\"Total tasks found: {len(list(tasks))}\")\n",
    "\n",
    "        print(f\"Available CPUs: {cpus}, num_cpus_per_task: {cpus_per_task}\")\n",
    "\n",
    "        remote_task = ray.remote(save_as_lerobot_dataset).options(num_cpus=cpus_per_task)\n",
    "        futures = []\n",
    "        \n",
    "        for task in tasks:\n",
    "            futures.append(\n",
    "                (task[0].stem, remote_task.remote(agibot_world_config, task, num_threads_per_task, save_depth, debug))\n",
    "            )\n",
    "\n",
    "        for task, future in futures:\n",
    "            try:\n",
    "                ray.get(future)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred for {task}\")\n",
    "                with open(\"output.txt\", \"a\") as f:\n",
    "                    f.write(f\"{task}, exception details: {str(e)}\\n\")\n",
    "\n",
    "        ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a19d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agibot_world_config={\n",
    "    \"images\": {\n",
    "        \"head\": {\n",
    "            \"dtype\": \"image\",\n",
    "            \"shape\": (720, 1280, 3),\n",
    "            \"names\": [\"height\", \"width\", \"rgb\"],\n",
    "        },\n",
    "        \"hand_left\": {\n",
    "            \"dtype\": \"image\",\n",
    "            \"shape\": (480, 848, 3),\n",
    "            \"names\": [\"height\", \"width\", \"rgb\"],\n",
    "        },\n",
    "        \"hand_right\": {\n",
    "            \"dtype\": \"image\",\n",
    "            \"shape\": (480, 848, 3),\n",
    "            \"names\": [\"height\", \"width\", \"rgb\"],\n",
    "        },\n",
    "    },\n",
    "    \"state\": {\n",
    "            \"dtype\": \"float32\",\n",
    "            \"shape\": (16,),\n",
    "            \"names\": {\n",
    "                \"motors\": [\n",
    "                    \"left_arm_0\",\n",
    "                    \"left_arm_1\",\n",
    "                    \"left_arm_2\",\n",
    "                    \"left_arm_3\", \n",
    "                    \"left_arm_4\",\n",
    "                    \"left_arm_5\",\n",
    "                    \"left_arm_6\",\n",
    "                    \"left_gripper\",\n",
    "                    \"right_arm_0\",\n",
    "                    \"right_arm_1\",\n",
    "                    \"right_arm_2\",\n",
    "                    \"right_arm_3\",\n",
    "                    \"right_arm_4\",\n",
    "                    \"right_arm_5\",\n",
    "                    \"right_arm_6\",\n",
    "                    \"right_gripper\",\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "    \"action\": {\n",
    "            \"dtype\": \"float32\",\n",
    "            \"shape\": (16,),\n",
    "            \"names\": {\n",
    "                \"motors\": [\n",
    "                    \"left_arm_0\",\n",
    "                    \"left_arm_1\",\n",
    "                    \"left_arm_2\",\n",
    "                    \"left_arm_3\",\n",
    "                    \"left_arm_4\",\n",
    "                    \"left_arm_5\",\n",
    "                    \"left_arm_6\",\n",
    "                    \"left_gripper\",\n",
    "                    \"right_arm_0\",\n",
    "                    \"right_arm_1\",\n",
    "                    \"right_arm_2\",\n",
    "                    \"right_arm_3\",\n",
    "                    \"right_arm_4\",\n",
    "                    \"right_arm_5\",\n",
    "                    \"right_arm_6\",\n",
    "                    \"right_gripper\",\n",
    "                ]\n",
    "            },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f5ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobids=[\n",
    "            \"12054834\",\n",
    "            \"12054877\",\n",
    "            \"12055136\",\n",
    "            \"12055201\",\n",
    "            \"12055201\",\n",
    "            \"12055225\",\n",
    "            \"12055366\",\n",
    "            \"12055366\",\n",
    "            \"12055373\",\n",
    "            \"12055399\",\n",
    "            \"12055409\",\n",
    "            \"12056273\",\n",
    "            \"12056267\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82239184",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = Path(\"/data/axgu/dataset/pack_in_the_supermarket\")\n",
    "output_path = Path(\"/data/axgu/dataset/pack_in_the_supermarket/lerobot_dataset\")\n",
    "eef_type = \"sim\"  # or \"gripper\", \"dexhand\", \"tactile\"\n",
    "cpus_per_task=10\n",
    "debug=True\n",
    "save_depth=False\n",
    "num_threads_per_task=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 其实一个task就是一个json文件 若干张图片和若干个h5文件\n",
    "# tasks=[\n",
    "#     (\"json_file\", \"local_dir\", \"h5_dir\", \"img_path\")\n",
    "# ]\n",
    "tasks = list(get_all_tasks_sim(src_path, output_path)) # 原来返回的是一次性迭代器\n",
    "print(f\"Total tasks found: {len(tasks)}\")\n",
    "save_as_lerobot_dataset(agibot_world_config,tasks[0], num_threads_per_task, save_depth, debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc591242",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = generate_features_from_config(agibot_world_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
